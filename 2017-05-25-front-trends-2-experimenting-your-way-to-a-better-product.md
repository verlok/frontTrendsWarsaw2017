---
layout: post
title: Experimenting your way to a better product - Zoe Mickley Gillenwater - Front-trends Warsaw 2017
date: 2017-05-24 18:00:00 +01:00
categories:
- conferences
tags: [front-trends, notes, __category__]
---

These are my notes from Front-trends 2017 Warsaw conference.

> We all want to create the best possible product for our users, that also meets the goals of our business in the strongest way. But how do we know if we are really doing that? A/B testing is a tool and process that can validate your design and development decisions through data, helping you remain focused on your customers and achieve more success in solving their problems.
> In this presentation, you’ll learn why A/B testing is important and why “expert” opinions about UX and visual design are so often wrong. I’ll talk about how Booking.com has been using A/B testing and other user research to optimize its products for a decade, and the do’s and don’t’s I’ve learned from running hundreds of experiments over the past three years there. You’ll leave with ideas for when and how to use data to improve your own work and add value to your business.


Experiment on your babies. :) They are all different, what works for one, doesnt for all.

Also our clients are all different. It's not one size fits all.

A/B testing is a method to experiment on your website to know what is best for your users.

Why you should consider it.

- Avoids HIPPOs
- Avoids bias and ego -- "I'm supposed to be an expert", but I'm not my users... our opinions and preferences might be wrong
- Users drive the product direction

We test everything ad booking.com

9 out of 10 tests fail. Like when you do user testing, they do things you didn't expect. (_yes, my father used to do this with my pet projects_)

Booking.com it's AB testing for over 8 years. That's a reason we're a leader ecommerce in the world.

## The process

- make observatins
- formulate hypothesis
- create experiment
- reject or accept hypothesis

Make observations: use data - like analytics

Data is fuel for hypothesis. You need to have access to it. Then you turn it into hypothesis. Break h.is in 4 parts: why / what / who / outcome

Why - _lost it_
What are yuo changing? - before and after
Who is affected? - whuch users and which conditions and which spots
Outcome - what you expect to happen to users, how to you measure failure of success, primary and supporting metrics, increase or decrease

(she does an example with butlers filter)

## How to do it

Create the smallest change possible, so you isolate tests

How long we need to run the test. We have a power calculators to know how many users and bla bla to test.

- existing conversion rate
- expected improvement
- number of variations
- average number of visitors
- statistical confidence 

--> 888888 visitors, 18 days

18 days we round it to 21 so we have 3 full weeks. -- The number will always be rounded up to weeks because days of the weeks have different types of users. So to have more accurate data.

So version A and B are different, the B have "find butlers".

Then you evaluate results.

Metrics combined together tell a story. If bookings go down, ... _lost it here, see slides_

You need a lot of context .. Metrics can become useless.

Increase usage of your feature doesn't mean it's good. Clicking more doesn't mean metter user experience.

Negative experiences are not bad. Don't descourage. We know that our experiments will fail. But we learn from them, we'll do best next time.

## Accept or reject hypothesis

Negtive or null responsive doesnt' necessarily mean "no".

The cycle is test, observe, hyphoth, __vedi foto__

## Don't test something just because you can

You should have a reason why you think that B is better then A. You could have different Bs... 

## Don't test the same thing over and over

A negative experiment could become positive. Example of cats, the old people we tested before became offended, they left and they're not coming back again. If I test it again we'll have different results. 

## Don't cherry pick metrics after running experiment

Texas sharpshooter fallacy --- draw the circle of target after shooting randomly :D

HARKing - hy.ing after results are known :D

## Don't AB test without enough traffic for significant results

No data isbetter than wrong data.

---

What to do if you get surprising results. Run the same test again, test the same concept somewhere else, validate with other data (analytics, etc.)

AB testing is better than not knowing, but you need to know what to test

Company cultur is important

Question everyone's opinion (including boss's)

Accept failure

Solve user problems! Make meaningful changes.
